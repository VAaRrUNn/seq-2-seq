{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "sns.set_theme(style=\"darkgrid\")\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN:\n",
    "    def __init__(self, x_embd, h_embd, y_embd, lr=1e-3):\n",
    "        self.x_embd = x_embd\n",
    "        self.h_embd = h_embd\n",
    "        self.y_embd = y_embd\n",
    "        self._initialize_parameters(x_embd, h_embd, y_embd)\n",
    "        self.lr = lr\n",
    "\n",
    "    def _initialize_parameters(self, \n",
    "                               x_embd,\n",
    "                               h_embd,\n",
    "                               y_embd):\n",
    "        \"\"\"\n",
    "        Initializing parameters for our RNN\n",
    "        \"\"\"\n",
    "        Wxh = np.random.randn(h_embd, x_embd) * 0.01\n",
    "        Whh = np.random.randn(h_embd, h_embd) * 0.01\n",
    "        Why = np.random.randn(y_embd, h_embd) * 0.01\n",
    "        bh = np.random.randn(h_embd, 1)\n",
    "        by = np.random.randn(y_embd, 1)\n",
    "\n",
    "        self.parameters = {\n",
    "            \"Wxh\": Wxh,\n",
    "            \"Whh\": Whh,\n",
    "            \"Why\": Why,\n",
    "            \"bh\": bh,\n",
    "            \"by\": by,\n",
    "        }\n",
    "\n",
    "        self.gradients = dict()\n",
    "\n",
    "        self.gradients[\"Wxh\"] = np.zeros_like(self.parameters[\"Wxh\"])\n",
    "        self.gradients[\"Why\"] = np.zeros_like(self.parameters[\"Why\"])\n",
    "        self.gradients[\"Whh\"] = np.zeros_like(self.parameters[\"Whh\"])\n",
    "        self.gradients[\"by\"] = np.zeros_like(self.parameters[\"by\"])\n",
    "        self.gradients[\"bh\"] = np.zeros_like(self.parameters[\"bh\"])\n",
    "        self.gradients[\"dnext_h\"] = np.zeros_like(self.parameters[\"bh\"])\n",
    "\n",
    "\n",
    "    def _softmax(self, x):\n",
    "        \"\"\"\n",
    "        x: numpy array of shape (-1, 1)\n",
    "        \"\"\"\n",
    "        temp = np.exp(x - np.max(x))\n",
    "        return temp/ temp.sum(axis=0)\n",
    "\n",
    "    def _mse(self, x):\n",
    "        pass\n",
    "    \n",
    "    def __one_step_forward(self, h_prev, x):\n",
    "        \"\"\"\n",
    "        Does one step forward for the RNN\n",
    "        Returns: logits\n",
    "        \"\"\"\n",
    "        h = np.tanh(np.dot(self.parameters[\"Wxh\"], x) + np.dot(self.parameters[\"Whh\"], h_prev) + self.parameters[\"bh\"]) # (h_embd, 1)\n",
    "        logits = np.dot(self.parameters[\"Why\"], h) + self.parameters[\"by\"] # (y_embd, 1)\n",
    "        # preds = self._softmax(logits)\n",
    "\n",
    "        return h, logits\n",
    "    \n",
    "    def forward(self, X, Y):\n",
    "        \"\"\"\n",
    "        Implementation of a full forward pass of the RNN\n",
    "        Args:\n",
    "            X: list of indices (C, 1), Here C is the nubmer of examples\n",
    "            Y: list of indices (C, 1)\n",
    "\n",
    "        \"\"\"\n",
    "        self.x, self.y_hat, self.h, self.y = {}, {}, {}, {}\n",
    "\n",
    "        # zeroth hidden state (h0)\n",
    "        self.h[-1] = np.zeros((self.h_embd, 1))\n",
    "\n",
    "        for i in range(len(X)):\n",
    "            self.x[i] = np.zeros((self.x_embd, 1))\n",
    "\n",
    "            # Make it a one-hot vector now\n",
    "            self.x[i][X[i]] = 1\n",
    "\n",
    "\n",
    "            # ----------------\n",
    "            # same for y\n",
    "            self.y[i] = np.zeros((self.y_embd, 1))\n",
    "            self.y[i][Y[i]] = 1\n",
    "            # -----------------\n",
    "\n",
    "\n",
    "            self.h[i], self.y_hat[i] = self.__one_step_forward(h_prev=self.h[i-1],\n",
    "                                                               x=self.x[i])\n",
    "            \n",
    "            # loss +=  Optional...\n",
    "\n",
    "    def _one_step_backward(self, x, h, h_prev, dy):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dy: dL/d y_pred\n",
    "        One backward step, and accumulation of the needed gradients\n",
    "        equations are:\n",
    "        \n",
    "        _h(t) = Wxh @ x + Whh @ h_prev + bh\n",
    "        h(t) = tanh(_h(t)) + dnext_h\n",
    "\n",
    "        # --------------------\n",
    "        ### IMPORTANT HERE\n",
    "        The dnext_h is the contribution of the next hidden state i.e., h(t+1)\n",
    "        Now the equations...\n",
    "        h(t+1) = Whh X h(t)\n",
    "        now we already know dL/ dh(t+1) which is simply dnext_h\n",
    "        so dh(t+1)/ dh(t) = Whh\n",
    "        now dL/ dh(t) = Whh X dnext_h (which is form the succeeding hidden layers.)\n",
    "        that's why we have to add it like this :).\n",
    "        # --------------------\n",
    "\n",
    "        y_pred(t) = Why @ h(t) + by\n",
    "\n",
    "        Loss = (y - y_pred) ** 2/2 (For simplicity rn)\n",
    "\n",
    "        Now for gradients:\n",
    "        \n",
    "        dL/d y_pred = (y - y_pred) -> (y_embd, 1)\n",
    "        dL/d Why = (y - y_pred) @ h(t).T -> (y_embd, 1) X (1, h_embd) -> (y_embd, h_embd)\n",
    "        dL/d by = (y - y_pred)\n",
    "\n",
    "        dL/d h(t) = Why.T @ (y - y_pred) -> (h_embd, y_embd) X (y_embd, 1) -> (h_embd, 1)\n",
    "        dL/d _h(t) = dL/ d h(t) * (1 - h(t) ** 2) -> (h_embd, 1)\n",
    "        dL/d Wxh = dL/d _h(t) @ x.T -> (h_embd, 1) @ (1, x_embd) -> (h_embd, x_embd)\n",
    "        dL/d bh = dL/d _h(t) -> (h_embd, 1)\n",
    "\n",
    "        dL/d Whh = dL/d _h(t) @ h_prev.T -> (h_embd, 1)\n",
    "\n",
    "        \"\"\"\n",
    "        self.gradients[\"Why\"] += dy @ h.T\n",
    "        self.gradients[\"by\"] += dy\n",
    "\n",
    "        dL_d_h = self.parameters[\"Why\"].T @ dy + self.parameters[\"Whh\"] @ self.gradients[\"dnext_h\"]\n",
    "        dL_d_h_ = dL_d_h * (1 - h**2) # _h(t)\n",
    "        self.gradients[\"Wxh\"] += dL_d_h_ @ x.T\n",
    "        self.gradients[\"Whh\"] += dL_d_h_ @ h_prev.T\n",
    "        self.gradients[\"bh\"] += dL_d_h_\n",
    "        self.gradients[\"dnext_h\"] = dL_d_h_\n",
    "\n",
    "    def rnn_backward(self, X, Y):\n",
    "        \"\"\"\n",
    "        Performs full BPT (Backpropagation through time)\n",
    "        Args:\n",
    "            X: list of indices (C, 1), Here C is the nubmer of examples\n",
    "            Y: list of indices (C, 1)\n",
    "        \"\"\"\n",
    "        for t in reversed(range(len(X))):\n",
    "            dy = self.y[t] - self.y_hat[t]\n",
    "            #  dy = dL/ dy_pred = (y - y_pred)\n",
    "\n",
    "            self._one_step_backward(x=self.x[t],\n",
    "                                    h=self.h[t],\n",
    "                                    h_prev=self.h[t-1],\n",
    "                                    dy=dy)\n",
    "            \n",
    "    def update_parameters(self, lr=None):\n",
    "        \"\"\"\n",
    "        Does simple SGD optimization\n",
    "        \"\"\"\n",
    "        if lr == None:\n",
    "            lr = self.lr\n",
    "        self.parameters[\"Wxh\"] += -lr * self.gradients[\"Wxh\"]\n",
    "        self.parameters[\"Whh\"] += -lr * self.gradients[\"Whh\"]\n",
    "        self.parameters[\"Why\"] += -lr * self.gradients[\"Why\"]\n",
    "        self.parameters[\"by\"] += -lr * self.gradients[\"by\"]\n",
    "        self.parameters[\"bh\"] += -lr * self.gradients[\"bh\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = RNN(x_embd=6,\n",
    "        h_embd=7,\n",
    "        y_embd=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [1, 3, 3]\n",
    "Y = [0, 2, 1]\n",
    "m.forward(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.rnn_backward(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.01020338, -0.01228859,  0.01589812,  0.01646996, -0.0136547 ,\n",
       "         0.01322151,  0.0056276 ],\n",
       "       [-0.0126856 ,  0.01523843, -0.01980315, -0.02049088,  0.01698445,\n",
       "        -0.01647989, -0.00700893],\n",
       "       [-0.0042678 ,  0.00512953, -0.00665962, -0.00689269,  0.00571348,\n",
       "        -0.00554126, -0.00235711],\n",
       "       [-0.00230374,  0.00275613, -0.00360689, -0.00372522,  0.00308666,\n",
       "        -0.00300465, -0.00127631],\n",
       "       [-0.00224441,  0.0027467 , -0.0034559 , -0.00360724,  0.00299489,\n",
       "        -0.00286218, -0.00122439],\n",
       "       [ 0.00460515, -0.00558828,  0.00713577,  0.00741846, -0.00615449,\n",
       "         0.00592295,  0.00252694],\n",
       "       [-0.02656744,  0.03174418, -0.04163379, -0.04297473,  0.03560432,\n",
       "        -0.03469316, -0.01473129]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.gradients[\"Whh\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
