{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "sns.set_theme(style=\"darkgrid\")\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2493403685.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[2], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    x: x_embd, 1\u001b[0m\n\u001b[1;37m             ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "x: x_embd, 1\n",
    "wah: h_dim, x_embd\n",
    "(wah dot x)\n",
    "\n",
    "whh: hdn, hdm \n",
    "why: y_dim, hdm \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN:\n",
    "    def __init__(self, x_embd, h_embd, y_embd, lr=1e-3):\n",
    "        self.x_embd = x_embd\n",
    "        self.h_embd = h_embd\n",
    "        self.y_embd = y_embd\n",
    "        self._initialize_parameters(x_embd, h_embd, y_embd)\n",
    "        self.lr = lr\n",
    "\n",
    "    def _initialize_parameters(self, \n",
    "                               x_embd,\n",
    "                               h_embd,\n",
    "                               y_embd):\n",
    "        \"\"\"\n",
    "        Initializing parameters for our RNN\n",
    "        \"\"\"\n",
    "        Wxh = np.random.randn(h_embd, x_embd) * 0.01\n",
    "        Whh = np.random.randn(h_embd, h_embd) * 0.01\n",
    "        Why = np.random.randn(y_embd, h_embd) * 0.01\n",
    "        bh = np.random.randn(h_embd, 1)\n",
    "        by = np.random.randn(y_embd, 1)\n",
    "\n",
    "        self.parameters = {\n",
    "            \"Wxh\": Wxh,\n",
    "            \"Whh\": Whh,\n",
    "            \"Why\": Why,\n",
    "            \"bh\": bh,\n",
    "            \"by\": by,\n",
    "        }\n",
    "\n",
    "        self.gradients = dict()\n",
    "\n",
    "        self.gradients[\"Wxh\"] = np.zeros_like(self.parameters[\"Wxh\"])\n",
    "        self.gradients[\"Why\"] = np.zeros_like(self.parameters[\"Why\"])\n",
    "        self.gradients[\"Whh\"] = np.zeros_like(self.parameters[\"Whh\"])\n",
    "        self.gradients[\"by\"] = np.zeros_like(self.parameters[\"by\"])\n",
    "        self.gradients[\"bh\"] = np.zeros_like(self.parameters[\"bh\"])\n",
    "        self.gradients[\"dnext_h\"] = np.zeros_like(self.parameters[\"bh\"])\n",
    "\n",
    "\n",
    "    def _softmax(self, x):\n",
    "        \"\"\"\n",
    "        x: numpy array of shape (-1, 1)\n",
    "        \"\"\"\n",
    "        temp = np.exp(x - np.max(x))\n",
    "        return temp/ temp.sum(axis=0)\n",
    "\n",
    "    def _mse(self, x):\n",
    "        pass\n",
    "    \n",
    "    def __one_step_forward(self, h_prev, x):\n",
    "        \"\"\"\n",
    "        Does one step forward for the RNN\n",
    "        Returns: logits\n",
    "        \"\"\"\n",
    "        h = np.tanh(np.dot(self.parameters[\"Wxh\"], x) + np.dot(self.parameters[\"Whh\"], h_prev) + self.parameters[\"bh\"]) # (h_embd, 1)\n",
    "        logits = np.dot(self.parameters[\"Why\"], h) + self.parameters[\"by\"] # (y_embd, 1)\n",
    "        # preds = self._softmax(logits)\n",
    "\n",
    "        return h, logits\n",
    "    \n",
    "    def forward(self, X, Y):\n",
    "        \"\"\"\n",
    "        Implementation of a full forward pass of the RNN\n",
    "        Args:\n",
    "            X: list of indices (C, 1), Here C is the nubmer of examples\n",
    "            Y: list of indices (C, 1)\n",
    "\n",
    "        \"\"\"\n",
    "        self.x, self.y_hat, self.h, self.y = {}, {}, {}, {}\n",
    "\n",
    "        # zeroth hidden state (h0)\n",
    "        self.h[-1] = np.zeros((self.h_embd, 1))\n",
    "\n",
    "        for i in range(len(X)):\n",
    "            self.x[i] = np.zeros((self.x_embd, 1))\n",
    "\n",
    "            # Make it a one-hot vector now\n",
    "            self.x[i][X[i]] = 1\n",
    "\n",
    "\n",
    "            # ----------------\n",
    "            # same for y\n",
    "            self.y[i] = np.zeros((self.y_embd, 1))\n",
    "            self.y[i][Y[i]] = 1\n",
    "            # -----------------\n",
    "\n",
    "\n",
    "            self.h[i], self.y_hat[i] = self.__one_step_forward(h_prev=self.h[i-1],\n",
    "                                                               x=self.x[i])\n",
    "            \n",
    "            # loss +=  Optional...\n",
    "\n",
    "    def _one_step_backward(self, x, h, h_prev, dy):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dy: dL/d y_pred\n",
    "        One backward step, and accumulation of the needed gradients\n",
    "        equations are:\n",
    "        \n",
    "        _h(t) = Wxh @ x + Whh @ h_prev + bh\n",
    "        h(t) = tanh(_h(t)) + dnext_h\n",
    "\n",
    "        # --------------------\n",
    "        ### IMPORTANT HERE\n",
    "        The dnext_h is the contribution of the next hidden state i.e., h(t+1)\n",
    "        Now the equations...\n",
    "        h(t+1) = Whh X h(t)\n",
    "        now we already know dL/ dh(t+1) which is simply dnext_h\n",
    "        so dh(t+1)/ dh(t) = Whh\n",
    "        now dL/ dh(t) = Whh X dnext_h (which is form the succeeding hidden layers.)\n",
    "        that's why we have to add it like this :).\n",
    "        # --------------------\n",
    "\n",
    "        y_pred(t) = Why @ h(t) + by\n",
    "\n",
    "        Loss = (y - y_pred) ** 2/2 (For simplicity rn)\n",
    "\n",
    "        Now for gradients:\n",
    "        \n",
    "        dL/d y_pred = (y - y_pred) -> (y_embd, 1)\n",
    "        dL/d Why = (y - y_pred) @ h(t).T -> (y_embd, 1) X (1, h_embd) -> (y_embd, h_embd)\n",
    "        dL/d by = (y - y_pred)\n",
    "\n",
    "        dL/d h(t) = Why.T @ (y - y_pred) -> (h_embd, y_embd) X (y_embd, 1) -> (h_embd, 1)\n",
    "        dL/d _h(t) = dL/ d h(t) * (1 - h(t) ** 2) -> (h_embd, 1)\n",
    "        dL/d Wxh = dL/d _h(t) @ x.T -> (h_embd, 1) @ (1, x_embd) -> (h_embd, x_embd)\n",
    "        dL/d bh = dL/d _h(t) -> (h_embd, 1)\n",
    "\n",
    "        dL/d Whh = dL/d _h(t) @ h_prev.T -> (h_embd, 1)\n",
    "\n",
    "        \"\"\"\n",
    "        self.gradients[\"Why\"] += dy @ h.T\n",
    "        self.gradients[\"by\"] += dy\n",
    "\n",
    "        dL_d_h = self.parameters[\"Why\"].T @ dy + self.parameters[\"Whh\"] @ self.gradients[\"dnext_h\"]\n",
    "        dL_d_h_ = dL_d_h * (1 - h**2) # _h(t)\n",
    "        self.gradients[\"Wxh\"] += dL_d_h_ @ x.T\n",
    "        self.gradients[\"Whh\"] += dL_d_h_ @ h_prev.T\n",
    "        self.gradients[\"bh\"] += dL_d_h_\n",
    "        self.gradients[\"dnext_h\"] = dL_d_h_\n",
    "\n",
    "    def rnn_backward(self, X, Y):\n",
    "        \"\"\"\n",
    "        Performs full BPT (Backpropagation through time)\n",
    "        Args:\n",
    "            X: list of indices (C, 1), Here C is the nubmer of examples\n",
    "            Y: list of indices (C, 1)\n",
    "        \"\"\"\n",
    "        for t in reversed(range(len(X))):\n",
    "            dy = self.y[t] - self.y_hat[t]\n",
    "            #  dy = dL/ dy_pred = (y - y_pred)\n",
    "\n",
    "            self._one_step_backward(x=self.x[t],\n",
    "                                    h=self.h[t],\n",
    "                                    h_prev=self.h[t-1],\n",
    "                                    dy=dy)\n",
    "            \n",
    "    def update_parameters(self, lr=None):\n",
    "        \"\"\"\n",
    "        Does simple SGD optimization\n",
    "        \"\"\"\n",
    "        if lr == None:\n",
    "            lr = self.lr\n",
    "        self.parameters[\"Wxh\"] += -lr * self.gradients[\"Wxh\"]\n",
    "        self.parameters[\"Whh\"] += -lr * self.gradients[\"Whh\"]\n",
    "        self.parameters[\"Why\"] += -lr * self.gradients[\"Why\"]\n",
    "        self.parameters[\"by\"] += -lr * self.gradients[\"by\"]\n",
    "        self.parameters[\"bh\"] += -lr * self.gradients[\"bh\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = RNN(x_embd=6,\n",
    "        h_embd=7,\n",
    "        y_embd=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [1, 3, 3]\n",
    "Y = [0, 2, 1]\n",
    "m.forward(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.rnn_backward(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.30189586e-03,  3.93118968e-02,  3.03283071e-02,\n",
       "        -7.44958060e-03,  3.93762113e-02, -8.64504447e-03,\n",
       "        -4.28397073e-02],\n",
       "       [-3.89463541e-04,  1.14861272e-02,  8.91501297e-03,\n",
       "        -2.17697413e-03,  1.15003535e-02, -2.49277095e-03,\n",
       "        -1.24978334e-02],\n",
       "       [-4.90307109e-04,  1.54427692e-02,  1.17888486e-02,\n",
       "        -2.92555801e-03,  1.54786526e-02, -3.47307680e-03,\n",
       "        -1.68728942e-02],\n",
       "       [ 2.23205830e-05, -3.24182703e-03, -1.99781533e-03,\n",
       "         6.10951372e-04, -3.28990287e-03,  1.02334208e-03,\n",
       "         3.71120698e-03],\n",
       "       [-9.76687698e-04,  2.88635707e-02,  2.23907771e-02,\n",
       "        -5.47045453e-03,  2.89003249e-02, -6.27139551e-03,\n",
       "        -3.14100873e-02],\n",
       "       [ 3.06151748e-04, -1.15918939e-02, -8.48291946e-03,\n",
       "         2.19357357e-03, -1.16499583e-02,  2.83294499e-03,\n",
       "         1.27952785e-02],\n",
       "       [-2.15324198e-04,  6.46085453e-03,  4.99245836e-03,\n",
       "        -1.22438194e-03,  6.47074077e-03, -1.41583812e-03,\n",
       "        -7.03779226e-03]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.gradients[\"Whh\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
